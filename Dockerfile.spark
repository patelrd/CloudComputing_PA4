FROM debian:slim

RUN apt-get -y update
RUN apt-get install -y default-jdk python3
RUN apt-get install -y python3-dev python3-pip
RUN apt-get update
RUN apt-get install -y python3-pip
RUN apt-get update
RUN apt-get install -y \
    python3 \
    python3-pip \
    openjdk-11-jdk \
    && rm -rf /var/lib/apt/lists/*

RUN pip3 install --no-cache-dir pyspark --break-system-packages

ENV JAVA_HOME /usr/lib/jvm/java-11-openjdk-amd64

RUN apt-get -y update
RUN apt-get install -y net-tools wget dnsutils iputils-ping iputils-tracepath iputils-arping iputils-clockdiff

#RUN apt-get install -y curl

#RUN wget https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3-scala2.13.tgz
#RUN apt-get update && apt-get install -y ca-certificates
RUN wget --timeout=60 --tries=10 -O spark-3.5.3-bin-hadoop3.2-scala2.13.tgz http://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3-scala2.13.tgz

RUN zcat spark-3.5.3-bin-hadoop3.2-scala2.13.tgz | tar xpof -

COPY spark-env.sh /spark-3.5.3-bin-hadoop3.2-scala2.13/conf/
COPY spark-worker.conf /spark-3.5.3-bin-hadoop3.2-scala2.13/conf/
COPY spark-driver.conf /spark-3.5.3-bin-hadoop3.2-scala2.13/conf/

RUN apt-get clean && rm -rf /var/lib/apt/lists/*

ENV SPARK_HOME=/spark-3.5.3-bin-hadoop3.2-scala2.13
ENV PATH=${PATH}:${SPARK_HOME}/sbin:${SPARK_HOME}/bin